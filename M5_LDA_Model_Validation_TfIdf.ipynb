{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus.reader.wordnet import WordNetError\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from collections import Counter\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Google navigation app\n",
    "- In this subset we explored Google’s application across languages other than \n",
    "Indonesian and English. \n",
    "- Since our entire date set includes reviews from 140 different countries, \n",
    "we were focused on 14 languages that captured the highest number of reviews \n",
    "after Indonesian and English. \n",
    "- Languages include Spanish, Chinese, Portuguese, Arabic, Russian, French, Thai, \n",
    "Italian, Turkish, German, Polish, USA, Vietnamese, Japanese, Chinese (Traditional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dianabursac/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/dianabursac/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dianabursac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dianabursac/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one time\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "final = pd.read_csv('final_2 4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['all_text_NOemoji'].fillna('NaN', inplace = True)\n",
    "final_all = final\n",
    "final = final.loc[final['all_text_NOemoji'] != 'NaN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "google = final.loc[final['App Name'] == 'Google Maps', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    38369\n",
       "1.0    14034\n",
       "4.0     8276\n",
       "3.0     6481\n",
       "2.0     4885\n",
       "Name: Rating, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google2 = google.loc[(google.Country != 'English') & (google.Country != 'Indonesian'), :]\n",
    "countries = google2.Country.value_counts().head(20) \n",
    "countriesIndx = countries[countries > 1000].index\n",
    "google_data2 = google2[google2.Country.isin(countriesIndx)]\n",
    "google_data2.Rating.value_counts().index.sort_values()\n",
    "google_data2.Rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/pandas/core/generic.py:6287: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "google_data2['cleaned_text'].fillna('NaN', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Pre-Processing\n",
    "- Stratified train test split was applied to ensure that the train and test sets \n",
    "have approximately the same percentage of different rating levels. \n",
    "- Since there is a large imbalance in the Google data subset across different ratings \n",
    "categories, the train data set was balanced by using random undersampling methodology. \n",
    "- As a result, Rating 1 and Rating 5 in the train data set contains an equal number \n",
    "of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomsampler(X, y):\n",
    "    data = pd.concat([X, y], axis = 1)\n",
    "    class_rating5 = data[data.Rating == 5.0]\n",
    "    class_others = data[data.Rating != 5.0]\n",
    "    r1, r2, r3, r4, r5 = data.Rating.value_counts().index.sort_values()\n",
    "    n1 = data.Rating.value_counts()[r1]\n",
    "    class_rating5_resample = class_rating5.sample(n1, random_state = 42)\n",
    "    new_data = pd.concat([class_others, class_rating5_resample], axis = 0)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = google_data2.drop([\"Rating\"], axis =1)\n",
    "y_data = google_data2[\"Rating\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data,random_state = 42,test_size =0.33, stratify = y_data)\n",
    "google_sampled_train = randomsampler(X_train, y_train)\n",
    "google_test = pd.concat([X_test, y_test], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Text Pre-processing, words filtering\n",
    "- General NLP techniques are used such as part of speech tagging, removal of standard English stop words, removal of the most common words, lemmatization\n",
    "- Stop words are excluded as well as additional words that didn’t provide insightful meaning such as : \n",
    "   \t'would’, 'also', 'see', 'something', 'please', \"everything”\n",
    "- In addition, we replaced words with similar meaning with a single word. For example: ‘perfect’, ‘excellent’, ‘great’, and ‘super’ were replaced with ‘excellent’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def count_most_common(p, vector):\n",
    "    count = Counter()\n",
    "    for line in vector:\n",
    "        count.update(line) \n",
    "    return count.most_common(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_all_words(vector):\n",
    "    count = Counter()\n",
    "    for line in vector:\n",
    "        count.update(line) \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = [nltk.word_tokenize(str(review)) for review in google_sampled_train['cleaned_text'].values]\n",
    "stop = stopwords.words('english')\n",
    "vectorNoStop = [[item for item in row if item not in stop] for row in vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = count_most_common(20, vectorNoStop)\n",
    "words_common, freq = zip(*most_common)\n",
    "# 1. stop words\n",
    "stop = stopwords.words('english')\n",
    "# 2. most common\n",
    "words_common = list(words_common)\n",
    "words_common, freq = zip(*most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_to_replace = [\n",
    "(('perfect', 'excellent',\"great\",'super', 'ممتاز', 'excellent_excellent','ممتاز','wonderful', 'great_app','best'),'great'), (('app', 'aplicativo', 'application'), 'app'),\\\n",
    "   (('good', 'cool', 'nice', 'fine', 'well', ' work_well', 'buena', 'good_good', 'muy buena'), 'good'), (('love', 'like', 'love_love'), 'love'), ('muito', 'much'),\\\n",
    "(('comment_comment', 'use_comment', 'comment_use', 'heard_comment', 'comment_know'), 'comment'), ('use_use', 'use'), ('open_open', \"open\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def replacement(vector):\n",
    "    for item0, item1 in items_to_replace:\n",
    "        if type(item0) is tuple:\n",
    "            for word in item0:\n",
    "                vector = [[item1 if item == word else item for item in line] for line in vector]\n",
    "        elif type(item0) is str:\n",
    "                vector = [[item1 if item == item0 else item for item in line] for line in vector]\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Identifying corpus and dictionary in LDA model\n",
    "\n",
    "- Two different values for min-frequency were explored: The dictionary with minimum \n",
    "word frequency equals one contains 19,948 unique words, while the dictionary with \n",
    "minimum words frequency equals three contains 5,463 unique words. \n",
    "- By setting min-frequency to 1 all the unique words from the training data set are captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_corpus(data, additional_stopwords, no_below, no_above):\n",
    "    vector = [nltk.word_tokenize(str(review)) for review in data.values]\n",
    "    vector_noStop = [[item for item in row if item not in stop and item not in additional_stopwords] for row in vector]\n",
    "    lemmas = [[lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in line] for line in vector_noStop]\n",
    "    vector_lemmas = replacement(lemmas)\n",
    "    vector_lemmas = [[item for item in line if len(item)>3] for line in vector_lemmas]\n",
    "    dictionary = corpora.Dictionary(vector_lemmas)\n",
    "    dictionary.filter_extremes(no_below, no_above)\n",
    "    corpus = [dictionary.doc2bow(text) for text in vector_lemmas]\n",
    "    print('Number of unique tokens: %d' % len(dictionary))\n",
    "    print('Number of documents: %d' % len(corpus))\n",
    "    return dictionary, corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_corpus_test(data, additional_stopwords,dictionary):\n",
    "    vector = [nltk.word_tokenize(str(review)) for review in data.values]\n",
    "    vector_noStop = [[item for item in row if item not in stop and item not in additional_stopwords] for row in vector]\n",
    "    lemmas = [[lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in line] for line in vector_noStop]\n",
    "    vector_lemmas = replacement(lemmas)\n",
    "    vector_lemmas = [[item for item in line if len(item)>3] for line in vector_lemmas]\n",
    "    corpus = [dictionary.doc2bow(text) for text in vector_lemmas]\n",
    "    print('Number of unique tokens: %d' % len(dictionary))\n",
    "    print('Number of documents: %d' % len(corpus))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = [nltk.word_tokenize(str(review)) for review in google_sampled_train['cleaned_text'].values]\n",
    "vector_lemmas = [[lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in line] for line in vector]\n",
    "vector_lemmas = [[item for item in line if len(item)>3] for line in vector_lemmas]\n",
    "most_common = count_most_common(30, vector_lemmas)\n",
    "words_common, freq = zip(*most_common)\n",
    "total = count_total_all_words(vector_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. additional_words and most common\n",
    "words1 = ['life', 'come', 'think', \"thank\",\"back\", 'yeah', 'chicken', \\\n",
    "          'would','also', 'see', 'something', 'please', \"everything\",\\\n",
    "          'feel', \"thing\", 'every', 'kind', 'google', \"possible\", 'really', 'among']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_common1 =set(list(words_common)+words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 19948\n",
      "Number of documents: 31966\n"
     ]
    }
   ],
   "source": [
    "dictionary1, corpus1 = prep_corpus(google_sampled_train['cleaned_text'],additional_stopwords = words_common1, no_below = 1, no_above=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 5463\n",
      "Number of documents: 31966\n"
     ]
    }
   ],
   "source": [
    "dictionary2, corpus2 = prep_corpus(google_sampled_train['cleaned_text'],additional_stopwords = words_common1, no_below = 3, no_above= 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build LDA models with 5 different topics\n",
    "- For each review in the train and the test data set we determined the Dominant Topic (out of 5 LDA topics), topic keywords and dominant topic % contribution\n",
    "- We use dictionary / corpus with 5463 unique tokens (min frequency = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, texts, ID, Rating_value,Reviews, app):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    reviewId = pd.Series(ID)\n",
    "    rating = pd.Series(Rating_value)\n",
    "    App = pd.Series(app)\n",
    "    reviews = pd.Series(Reviews)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents, reviewId, rating, App, reviews], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_all = [[dictionary2[id] for id, freq in line] for line in corpus2]\n",
    "reviewID = [id  for id in google_sampled_train['Review ID'].values]\n",
    "Rating = [r  for r in google_sampled_train['Rating'].values]\n",
    "appName = [name for name in google_sampled_train[\"App Name\"].values]\n",
    "reviews = [review for review in google_sampled_train['cleaned_text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 5463\n",
      "Number of documents: 23775\n"
     ]
    }
   ],
   "source": [
    "test_corpus2 = prep_corpus_test(google_test['cleaned_text'], additional_stopwords = words_common1, dictionary= dictionary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_all_T = [[dictionary2[id] for id, freq in line] for line in test_corpus2]\n",
    "reviewID_T = [id  for id in google_test['Review ID'].values ]\n",
    "Rating_T = [r  for r in google_test['Rating'].values]\n",
    "appName_T = [name for name in google_test[\"App Name\"].values]\n",
    "reviews_T = [review for review in google_test['cleaned_text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA model with 5 topics\n",
    "\n",
    "lda5S = models.ldamodel.LdaModel(corpus=corpus2, id2word=dictionary2, num_topics=5, passes=20,chunksize=4000,random_state=43)\n",
    "topics5S = lda5S.print_topics(num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 topics train Small data set \n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda5S, corpus=corpus2, texts=text_all, ID = reviewID, Rating_value = Rating, app = appName, Reviews = reviews)\n",
    "df_dominant_topic5S = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic5S.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text', 'Review ID', \"Rating\", \"App Name\", \"Review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 topics test Small data set \n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda5S, corpus=test_corpus2, texts=text_all_T, ID = reviewID_T, Rating_value = Rating_T, app = appName_T, Reviews = reviews_T)\n",
    "df_dominant_topic5S_T = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic5S_T.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text', 'Review ID', \"Rating\", \"App Name\", \"Review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation of LDA using supervised leraning\n",
    "- Conversion to document term matrices was employed on the corpus with min frequency = 3 (5463 unique words)\n",
    "- Validation of LDA model with 5 topics was done on the both train and test data sets by \n",
    "using tfidf matrices and Dominant Topic as a target variable\n",
    "- SVM and NB were used for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Xtfidf(min, max, train, test):\n",
    "    cv = TfidfVectorizer(sublinear_tf = True, min_df = min, max_df = max, norm='l2', binary = True, use_idf = True) \n",
    "    X_train = cv.fit_transform(train)\n",
    "    X_test = cv.transform(test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_corpus_tfidf(data, additional_stopwords):\n",
    "    vector = [nltk.word_tokenize(str(review)) for review in data.values]\n",
    "    vector_noStop = [[item for item in row if item not in stop and item not in additional_stopwords] for row in vector]\n",
    "    lemmas = [[lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in line] for line in vector_noStop]\n",
    "    vector_lemmas = replacement(lemmas)\n",
    "    vector_lemmas = [[item for item in line if len(item)>3] for line in vector_lemmas]\n",
    "    return vector_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_train5S = prep_corpus_tfidf(df_dominant_topic5S['Review'], additional_stopwords = words_common1)\n",
    "lemmas_test5S = prep_corpus_tfidf(df_dominant_topic5S_T['Review'], additional_stopwords = words_common1)\n",
    "data_train5S = [' '.join(x) for x in lemmas_train5S]\n",
    "data_test5S = [' '.join(x) for x in lemmas_test5S]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train5S, X_test5S = create_Xtfidf(min =3, max = 500, train = data_train5S, test = data_test5S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_score(X_train, X_test, y_train, y_test):\n",
    "    clf_svm = SGDClassifier(loss='hinge', penalty='l2',alpha=0.01, random_state=42)\n",
    "    clf_svm.fit(X_train, y_train) \n",
    "    y_train_pred= clf_svm.predict(X_train)\n",
    "    y_test_pred= clf_svm.predict(X_test)\n",
    "    f_test = f1_score(y_test, y_test_pred, average='micro')\n",
    "    f_train = f1_score(y_train, y_train_pred, average='micro')\n",
    "    return f_train, f_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def tfidf_scoreReport_NB(X_train, X_test, y_train, y_test):\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train, y_train) \n",
    "    y_train_pred= clf.predict(X_train)\n",
    "    y_test_pred= clf.predict(X_test)\n",
    "    report = classification_report(y_test,y_test_pred)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_scoreReport_SV(X_train, X_test, y_train, y_test):\n",
    "    clf = SGDClassifier(loss='hinge', penalty='l2',alpha=0.01, random_state=42)\n",
    "    clf.fit(X_train, y_train) \n",
    "    y_train_pred= clf.predict(X_train)\n",
    "    y_test_pred= clf.predict(X_test)\n",
    "    report = classification_report(y_test,y_test_pred)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_scoreNB(X_train, X_test, y_train, y_test):\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train, y_train) \n",
    "    y_train_pred= clf.predict(X_train)\n",
    "    y_test_pred= clf.predict(X_test)\n",
    "    f_test = f1_score(y_test, y_test_pred, average='micro')\n",
    "    f_train = f1_score(y_train, y_train_pred, average='micro')\n",
    "    return f_train, f_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_train5S = np.array(df_dominant_topic5S['Dominant_Topic'])\n",
    "y_test5S = np.array(df_dominant_topic5S_T['Dominant_Topic'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f5_tf_S, f5T_tf_S = tfidf_score(X_train5S, X_test5S, y_train5S, y_test5S)\n",
    "f5_tf_S_nb, f5T_tf_S_nb = tfidf_scoreNB(X_train5S, X_test5S, y_train5S, y_test5S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7065006569480072, 0.6731440588853838)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f5_tf_S, f5T_tf_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
